{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1      2      3       4       5        6        7        8   \\\n",
       "0      842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df_2 = pd.read_csv('breast-cancer.csv',header=None)\n",
    "heart_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the field names from the text file and adding them as column names of the loaded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>radius_sd_error</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>texture_sd_error</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>perimeter_sd_error</th>\n",
       "      <th>...</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>concave_points_sd_error</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>symmetry_sd_error</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>fractal_dimension_sd_error</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0          ID diagnosis  radius_mean  radius_sd_error  radius_worst  \\\n",
       "0      842302         M        17.99            10.38        122.80   \n",
       "1      842517         M        20.57            17.77        132.90   \n",
       "2    84300903         M        19.69            21.25        130.00   \n",
       "3    84348301         M        11.42            20.38         77.58   \n",
       "4    84358402         M        20.29            14.34        135.10   \n",
       "..        ...       ...          ...              ...           ...   \n",
       "564    926424         M        21.56            22.39        142.00   \n",
       "565    926682         M        20.13            28.25        131.20   \n",
       "566    926954         M        16.60            28.08        108.30   \n",
       "567    927241         M        20.60            29.33        140.10   \n",
       "568     92751         B         7.76            24.54         47.92   \n",
       "\n",
       "0    texture_mean  texture_sd_error  texture_worst  perimeter_mean  \\\n",
       "0          1001.0           0.11840        0.27760         0.30010   \n",
       "1          1326.0           0.08474        0.07864         0.08690   \n",
       "2          1203.0           0.10960        0.15990         0.19740   \n",
       "3           386.1           0.14250        0.28390         0.24140   \n",
       "4          1297.0           0.10030        0.13280         0.19800   \n",
       "..            ...               ...            ...             ...   \n",
       "564        1479.0           0.11100        0.11590         0.24390   \n",
       "565        1261.0           0.09780        0.10340         0.14400   \n",
       "566         858.1           0.08455        0.10230         0.09251   \n",
       "567        1265.0           0.11780        0.27700         0.35140   \n",
       "568         181.0           0.05263        0.04362         0.00000   \n",
       "\n",
       "0    perimeter_sd_error  ...  concavity_worst  concave_points_mean  \\\n",
       "0               0.14710  ...           25.380                17.33   \n",
       "1               0.07017  ...           24.990                23.41   \n",
       "2               0.12790  ...           23.570                25.53   \n",
       "3               0.10520  ...           14.910                26.50   \n",
       "4               0.10430  ...           22.540                16.67   \n",
       "..                  ...  ...              ...                  ...   \n",
       "564             0.13890  ...           25.450                26.40   \n",
       "565             0.09791  ...           23.690                38.25   \n",
       "566             0.05302  ...           18.980                34.12   \n",
       "567             0.15200  ...           25.740                39.42   \n",
       "568             0.00000  ...            9.456                30.37   \n",
       "\n",
       "0    concave_points_sd_error  concave_points_worst  symmetry_mean  \\\n",
       "0                     184.60                2019.0        0.16220   \n",
       "1                     158.80                1956.0        0.12380   \n",
       "2                     152.50                1709.0        0.14440   \n",
       "3                      98.87                 567.7        0.20980   \n",
       "4                     152.20                1575.0        0.13740   \n",
       "..                       ...                   ...            ...   \n",
       "564                   166.10                2027.0        0.14100   \n",
       "565                   155.00                1731.0        0.11660   \n",
       "566                   126.70                1124.0        0.11390   \n",
       "567                   184.60                1821.0        0.16500   \n",
       "568                    59.16                 268.6        0.08996   \n",
       "\n",
       "0    symmetry_sd_error  symmetry_worst  fractal_dimension_mean  \\\n",
       "0              0.66560          0.7119                  0.2654   \n",
       "1              0.18660          0.2416                  0.1860   \n",
       "2              0.42450          0.4504                  0.2430   \n",
       "3              0.86630          0.6869                  0.2575   \n",
       "4              0.20500          0.4000                  0.1625   \n",
       "..                 ...             ...                     ...   \n",
       "564            0.21130          0.4107                  0.2216   \n",
       "565            0.19220          0.3215                  0.1628   \n",
       "566            0.30940          0.3403                  0.1418   \n",
       "567            0.86810          0.9387                  0.2650   \n",
       "568            0.06444          0.0000                  0.0000   \n",
       "\n",
       "0    fractal_dimension_sd_error  fractal_dimension_worst  \n",
       "0                        0.4601                  0.11890  \n",
       "1                        0.2750                  0.08902  \n",
       "2                        0.3613                  0.08758  \n",
       "3                        0.6638                  0.17300  \n",
       "4                        0.2364                  0.07678  \n",
       "..                          ...                      ...  \n",
       "564                      0.2060                  0.07115  \n",
       "565                      0.2572                  0.06637  \n",
       "566                      0.2218                  0.07820  \n",
       "567                      0.4087                  0.12400  \n",
       "568                      0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df=heart_df_2.copy()\n",
    "field_names = pd.read_csv('field_names.txt',header=None)\n",
    "heart_df.columns=field_names[0]\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data types of the loaded dataset and converting as required. For our ML modelling purpose we will require the diagnosis to be in the type of integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ID                              int64\n",
      "diagnosis                      object\n",
      "radius_mean                   float64\n",
      "radius_sd_error               float64\n",
      "radius_worst                  float64\n",
      "texture_mean                  float64\n",
      "texture_sd_error              float64\n",
      "texture_worst                 float64\n",
      "perimeter_mean                float64\n",
      "perimeter_sd_error            float64\n",
      "perimeter_worst               float64\n",
      "area_mean                     float64\n",
      "area_sd_error                 float64\n",
      "area_worst                    float64\n",
      "smoothness_mean               float64\n",
      "smoothness_sd_error           float64\n",
      "smoothness_worst              float64\n",
      "compactness_mean              float64\n",
      "compactness_sd_error          float64\n",
      "compactness_worst             float64\n",
      "concavity_mean                float64\n",
      "concavity_sd_error            float64\n",
      "concavity_worst               float64\n",
      "concave_points_mean           float64\n",
      "concave_points_sd_error       float64\n",
      "concave_points_worst          float64\n",
      "symmetry_mean                 float64\n",
      "symmetry_sd_error             float64\n",
      "symmetry_worst                float64\n",
      "fractal_dimension_mean        float64\n",
      "fractal_dimension_sd_error    float64\n",
      "fractal_dimension_worst       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(heart_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID column is a unique columns or a name column and is non informative. Thus we will remove this from our data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del heart_df['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape of the loaded dataset to make sure the dataset has been loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will integer encode the target variable (in our case diagnosis). We will replace 'M' as 1 as we will move forward modelling malignant tumors and 'B' as 0. After replacing we can see that there are 357 Benign and 212 Malign diagnosis in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    357\n",
       "1    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_mapping= {'M': 1, 'B': 0}\n",
    "heart_df['diagnosis'] = heart_df['diagnosis'].replace(level_mapping)\n",
    "heart_df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values in the dataset. Since there are no null values present no action is tobe taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "diagnosis                     0\n",
      "radius_mean                   0\n",
      "radius_sd_error               0\n",
      "radius_worst                  0\n",
      "texture_mean                  0\n",
      "texture_sd_error              0\n",
      "texture_worst                 0\n",
      "perimeter_mean                0\n",
      "perimeter_sd_error            0\n",
      "perimeter_worst               0\n",
      "area_mean                     0\n",
      "area_sd_error                 0\n",
      "area_worst                    0\n",
      "smoothness_mean               0\n",
      "smoothness_sd_error           0\n",
      "smoothness_worst              0\n",
      "compactness_mean              0\n",
      "compactness_sd_error          0\n",
      "compactness_worst             0\n",
      "concavity_mean                0\n",
      "concavity_sd_error            0\n",
      "concavity_worst               0\n",
      "concave_points_mean           0\n",
      "concave_points_sd_error       0\n",
      "concave_points_worst          0\n",
      "symmetry_mean                 0\n",
      "symmetry_sd_error             0\n",
      "symmetry_worst                0\n",
      "fractal_dimension_mean        0\n",
      "fractal_dimension_sd_error    0\n",
      "fractal_dimension_worst       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(heart_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to remove outliers.Since the data is that of diagnosis of tumor we cannot really say if the bigger values in a column are outliers or not. Thus for this modelling purpose we will go with the same data we have without removing outliers. If the information is given or known we can go forward with removing outliers which are 3-4 standard deviations away by either dropping these rows or imputing the values with a mean or median value. For this dataset we will remove the rows with very far away values only (6 or 7 standard deviations away from the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "z_scores = stats.zscore(heart_df)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 7).all(axis=1)\n",
    "new_heart_df = heart_df[filtered_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The malign and benign counts of diagnosis after removing very far outliers are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    354\n",
       "1    208\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_heart_df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY ANALYSYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(new_heart_df['diagnosis'], new_heart_df['smoothness_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt=sns.distplot(new_heart_df['smoothness_mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_heart_df['smoothness_mean'].hist(by=new_heart_df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Behaviour = new_heart_df['diagnosis'].value_counts(normalize=False).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(new_heart_df['radius_mean'], new_heart_df['texture_mean'], hue = new_heart_df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(new_heart_df['diagnosis'], new_heart_df['perimeter_mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_plot = sns.FacetGrid(data=new_heart_df, hue='diagnosis')\n",
    "cond_plot.map(sns.scatterplot, 'fractal_dimension_mean', 'perimeter_mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_plot = sns.FacetGrid(data=new_heart_df, hue='diagnosis')\n",
    "cond_plot.map(sns.scatterplot, 'concave_points_sd_error', 'radius_mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(new_heart_df.corr(),cmap=\"GnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do modeling on our dataset, we break the dataset into a dataset named 'data' which contains all the descriptive features and anther dataset named 'target' containing all the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_heart_df.drop(columns='diagnosis')\n",
    "target = new_heart_df['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of dataframe with descriptive variables as 'data' and the diagnosis as 'target' are as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data\n",
      "(562, 30)\n",
      "Shape of target\n",
      "(562,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data\")\n",
    "print(data.shape)\n",
    "print(\"Shape of target\")\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now scale all the desriptive variables using the min-max scaler for making it easier to model our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 random rows of Dataframe after scaling\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>radius_sd_error</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>texture_sd_error</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>perimeter_sd_error</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>concave_points_sd_error</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>symmetry_sd_error</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>fractal_dimension_sd_error</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.528633</td>\n",
       "      <td>0.373013</td>\n",
       "      <td>0.532210</td>\n",
       "      <td>0.389366</td>\n",
       "      <td>0.531462</td>\n",
       "      <td>0.503801</td>\n",
       "      <td>0.434630</td>\n",
       "      <td>0.523857</td>\n",
       "      <td>0.460101</td>\n",
       "      <td>0.246420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497619</td>\n",
       "      <td>0.348881</td>\n",
       "      <td>0.461121</td>\n",
       "      <td>0.343969</td>\n",
       "      <td>0.466420</td>\n",
       "      <td>0.312901</td>\n",
       "      <td>0.306239</td>\n",
       "      <td>0.520619</td>\n",
       "      <td>0.177016</td>\n",
       "      <td>0.146530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>0.157073</td>\n",
       "      <td>0.334122</td>\n",
       "      <td>0.151399</td>\n",
       "      <td>0.079848</td>\n",
       "      <td>0.430351</td>\n",
       "      <td>0.190603</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.055467</td>\n",
       "      <td>0.369192</td>\n",
       "      <td>0.281171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.289446</td>\n",
       "      <td>0.097658</td>\n",
       "      <td>0.049926</td>\n",
       "      <td>0.365383</td>\n",
       "      <td>0.089948</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.076701</td>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.081202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.447107</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>0.448341</td>\n",
       "      <td>0.308426</td>\n",
       "      <td>0.381692</td>\n",
       "      <td>0.403123</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.349950</td>\n",
       "      <td>0.364646</td>\n",
       "      <td>0.206403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429762</td>\n",
       "      <td>0.265458</td>\n",
       "      <td>0.413047</td>\n",
       "      <td>0.272514</td>\n",
       "      <td>0.477646</td>\n",
       "      <td>0.407981</td>\n",
       "      <td>0.423590</td>\n",
       "      <td>0.680756</td>\n",
       "      <td>0.286615</td>\n",
       "      <td>0.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.289985</td>\n",
       "      <td>0.394319</td>\n",
       "      <td>0.280818</td>\n",
       "      <td>0.176122</td>\n",
       "      <td>0.206554</td>\n",
       "      <td>0.218410</td>\n",
       "      <td>0.143533</td>\n",
       "      <td>0.092793</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.235468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256746</td>\n",
       "      <td>0.399520</td>\n",
       "      <td>0.230421</td>\n",
       "      <td>0.141863</td>\n",
       "      <td>0.150895</td>\n",
       "      <td>0.161355</td>\n",
       "      <td>0.157094</td>\n",
       "      <td>0.192474</td>\n",
       "      <td>0.181944</td>\n",
       "      <td>0.173619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.083502</td>\n",
       "      <td>0.160298</td>\n",
       "      <td>0.076856</td>\n",
       "      <td>0.039734</td>\n",
       "      <td>0.349824</td>\n",
       "      <td>0.080063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334343</td>\n",
       "      <td>0.363943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.044441</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>0.297365</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202444</td>\n",
       "      <td>0.153745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0    radius_mean  radius_sd_error  radius_worst  texture_mean  \\\n",
       "153     0.528633         0.373013      0.532210      0.389366   \n",
       "549     0.157073         0.334122      0.151399      0.079848   \n",
       "324     0.447107         0.196145      0.448341      0.308426   \n",
       "401     0.289985         0.394319      0.280818      0.176122   \n",
       "172     0.083502         0.160298      0.076856      0.039734   \n",
       "\n",
       "0    texture_sd_error  texture_worst  perimeter_mean  perimeter_sd_error  \\\n",
       "153          0.531462       0.503801        0.434630            0.523857   \n",
       "549          0.430351       0.190603        0.011774            0.055467   \n",
       "324          0.381692       0.403123        0.282099            0.349950   \n",
       "401          0.206554       0.218410        0.143533            0.092793   \n",
       "172          0.349824       0.080063        0.000000            0.000000   \n",
       "\n",
       "0    perimeter_worst  area_mean  ...  concavity_worst  concave_points_mean  \\\n",
       "153         0.460101   0.246420  ...         0.497619             0.348881   \n",
       "549         0.369192   0.281171  ...         0.107937             0.289446   \n",
       "324         0.364646   0.206403  ...         0.429762             0.265458   \n",
       "401         0.262626   0.235468  ...         0.256746             0.399520   \n",
       "172         0.334343   0.363943  ...         0.052857             0.133795   \n",
       "\n",
       "0    concave_points_sd_error  concave_points_worst  symmetry_mean  \\\n",
       "153                 0.461121              0.343969       0.466420   \n",
       "549                 0.097658              0.049926       0.365383   \n",
       "324                 0.413047              0.272514       0.477646   \n",
       "401                 0.230421              0.141863       0.150895   \n",
       "172                 0.044441              0.022792       0.297365   \n",
       "\n",
       "0    symmetry_sd_error  symmetry_worst  fractal_dimension_mean  \\\n",
       "153           0.312901        0.306239                0.520619   \n",
       "549           0.089948        0.008590                0.076701   \n",
       "324           0.407981        0.423590                0.680756   \n",
       "401           0.161355        0.157094                0.192474   \n",
       "172           0.041990        0.000000                0.000000   \n",
       "\n",
       "0    fractal_dimension_sd_error  fractal_dimension_worst  \n",
       "153                    0.177016                 0.146530  \n",
       "549                    0.137394                 0.081202  \n",
       "324                    0.286615                 0.237439  \n",
       "401                    0.181944                 0.173619  \n",
       "172                    0.202444                 0.153745  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "data_df=data.copy()\n",
    "Data_scaler = preprocessing.MinMaxScaler()\n",
    "Data_scaler.fit(data)\n",
    "data = Data_scaler.fit_transform(data)\n",
    "print(\"5 random rows of Dataframe after scaling\")\n",
    "pd.DataFrame(data, columns=data_df.columns).sample(5, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the best feature selection method which can be used on our data. We will use the roc_auc value score by fitting  on the K nearest neighbour estimator using different feature selecting technique. We cross validate on RepeatedStratifiedKFold with number of splits=5 and a repeatition of 3 times.The feature selection we will be using are \\\n",
    "Full set of features \\\n",
    "F-score method \\\n",
    "Mutual information method \\\n",
    "\n",
    "We will then check the performance comparison of these models by paired t-test and select the best feature selection method.This will then be used in pipeline for our modelling purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc for full set of features\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "#full set of features\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3,random_state=999)\n",
    "cv_results_full = cross_val_score(estimator=KNeighborsClassifier(n_neighbors=20),\n",
    "                                  X=data,\n",
    "                                  y=target,\n",
    "                                  cv=cv_method,\n",
    "                                  scoring='roc_auc')\n",
    "print(\"roc_auc for full set of features\")\n",
    "print(cv_results_full.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc for f-score method\n",
      "0.981\n"
     ]
    }
   ],
   "source": [
    "# f-score method\n",
    "from sklearn import feature_selection as fs\n",
    "fs_fit_fscore = fs.SelectKBest(fs.f_classif, k=10)\n",
    "fs_fit_fscore.fit_transform(data, target)\n",
    "fs_indices_fscore = np.argsort(np.nan_to_num(fs_fit_fscore.scores_))[::-1][0:10]\n",
    "best_features_fscore = data_df.columns[fs_indices_fscore].values\n",
    "feature_importances_fscore = fs_fit_fscore.scores_[fs_indices_fscore]\n",
    "cv_results_fscore = cross_val_score(estimator=KNeighborsClassifier(n_neighbors=20),\n",
    "                                    X=data[:, fs_indices_fscore],\n",
    "                                    y=target,\n",
    "                                    cv=cv_method,\n",
    "                                    scoring='roc_auc')\n",
    "print(\"roc_auc for f-score method\")\n",
    "print(cv_results_fscore.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc value for mutual information method\n",
      "0.984\n"
     ]
    }
   ],
   "source": [
    "#mutual information method\n",
    "fs_fit_mutual_info = fs.SelectKBest(fs.mutual_info_classif, k=10)\n",
    "fs_fit_mutual_info.fit_transform(data, target)\n",
    "fs_indices_mutual_info = np.argsort(fs_fit_mutual_info.scores_)[::-1][0:10]\n",
    "best_features_mutual_info = data_df.columns[fs_indices_mutual_info].values\n",
    "feature_importances_mutual_info = fs_fit_mutual_info.scores_[fs_indices_mutual_info]\n",
    "cv_results_mutual_info = cross_val_score(estimator=KNeighborsClassifier(n_neighbors=20),\n",
    "                                         X=data[:, fs_indices_mutual_info],\n",
    "                                         y=target,\n",
    "                                         cv=cv_method,\n",
    "                                         scoring='roc_auc')\n",
    "print(\"roc_auc value for mutual information method\")\n",
    "print(cv_results_mutual_info.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roc_auc values for the 3 different feature selection method we have used with the same cross validation method,scoring method,and estimator is as given below.The higher value is that when using the full set of features.We use paired t-test to compare compare it to the other feature selection method results, to check if it is statistically signifant than the other 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Set of Features: 0.99\n",
      "F-Score: 0.981\n",
      "Mutual Information: 0.984\n",
      "paired t-test comparison values of mutual information method with others\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print('Full Set of Features:', cv_results_full.mean().round(3))\n",
    "print('F-Score:', cv_results_fscore.mean().round(3))\n",
    "print('Mutual Information:', cv_results_mutual_info.mean().round(3))\n",
    "print(\"paired t-test comparison values of mutual information method with others\")\n",
    "print(stats.ttest_rel(cv_results_full, cv_results_mutual_info).pvalue.round(3))\n",
    "print(stats.ttest_rel(cv_results_full, cv_results_fscore).pvalue.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the paired t-test returns a value which is less that 0.05 and thus we can say that using full feature set is statistically significant than choosing the other two methods. Thus for our modelling purpose we will go with the full descriptive feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split our feature dataset and the target dataset into train and test partitions with a 7:3 ratio. We will be training the model on our train split datasets and testing it on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature dataset\n",
      "(393, 30)\n",
      "Shape of testing feature dataset\n",
      "(169, 30)\n",
      "Shape of training target dataset\n",
      "(393,)\n",
      "Shape of testing feature dataset\n",
      "(169,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Data_train, Data_test, \\\n",
    "target_train, target_test = train_test_split(data, target, \n",
    "                                                    test_size = 0.3, random_state=999,\n",
    "                                                    stratify = target)\n",
    "print(\"Shape of training feature dataset\")\n",
    "print(Data_train.shape)\n",
    "print(\"Shape of testing feature dataset\")\n",
    "print(Data_test.shape)\n",
    "print(\"Shape of training target dataset\")\n",
    "print(target_train.shape)\n",
    "print(\"Shape of testing feature dataset\")\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our modelling purpose we will use a 5 fold stratified cross validation method with a repetition of 3 times to train the model and hypertune the various model parameters. This will reduce the issues of overfitting of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target variable (diagnosis) is as below and can be observed that it is unevenly distributed. In such a case it is better use aus_roc scoring method for performance comparison of our model. Thus we will go forward with a performance scoring of auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.631043\n",
       "1    0.368957\n",
       "Name: diagnosis, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Tuning Parameters of Different Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors(KNN) Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 21 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 315 out of 315 | elapsed:   33.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe_KNN = Pipeline([('fselector', SelectKBest(fs.mutual_info_classif)), \n",
    "                     ('knn', KNeighborsClassifier())])\n",
    "\n",
    "params_pipe_KNN = {'fselector__k': [data.shape[1]],\n",
    "                   'knn__n_neighbors': [1, 5, 10, 15,30,50,70],\n",
    "                   'knn__p': [1, 2,5]}\n",
    "\n",
    "gs_pipe_KNN = GridSearchCV(estimator=pipe_KNN, \n",
    "                           param_grid=params_pipe_KNN, \n",
    "                           cv=cv_method,\n",
    "                           scoring='roc_auc',\n",
    "                           verbose=1) \n",
    "\n",
    "gs_pipe_KNN.fit(Data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of parameters using KNN model\n",
      "{'fselector__k': 30, 'knn__n_neighbors': 30, 'knn__p': 2}\n",
      "roc_auc value fitting with best parameters\n",
      "0.9936575181796855\n"
     ]
    }
   ],
   "source": [
    "print(\"Best set of parameters using KNN model\")\n",
    "print(gs_pipe_KNN.best_params_)\n",
    "print(\"roc_auc value fitting with best parameters\")\n",
    "print(gs_pipe_KNN.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes(NB) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 20 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-2)]: Done 300 out of 300 | elapsed:   17.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "Data_train_transformed = PowerTransformer().fit_transform(Data_train)\n",
    "\n",
    "pipe_NB = Pipeline([('fselector', SelectKBest(fs.mutual_info_classif)), \n",
    "                     ('nb', GaussianNB())])\n",
    "\n",
    "params_pipe_NB = {'fselector__k': [data.shape[1]],\n",
    "                  'nb__var_smoothing': np.logspace(1,-3, num=20)}\n",
    "\n",
    "gs_pipe_NB = GridSearchCV(estimator=pipe_NB, \n",
    "                          param_grid=params_pipe_NB, \n",
    "                          cv=cv_method,\n",
    "                          refit=True,\n",
    "                          n_jobs=-2,\n",
    "                          scoring='roc_auc',\n",
    "                          verbose=1) \n",
    "\n",
    "gs_pipe_NB.fit(Data_train_transformed, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of parameters using NB model\n",
      "{'fselector__k': 30, 'nb__var_smoothing': 0.011288378916846895}\n",
      "roc_auc value fitting with best parameters\n",
      "0.9882120572366877\n"
     ]
    }
   ],
   "source": [
    "print(\"Best set of parameters using NB model\")\n",
    "print(gs_pipe_NB.best_params_)\n",
    "print(\"roc_auc value fitting with best parameters\")\n",
    "print(gs_pipe_NB.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree(DT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 50 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  82 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-2)]: Done 382 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-2)]: Done 750 out of 750 | elapsed:   41.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipe_DT = Pipeline([('fselector', SelectKBest(fs.mutual_info_classif)),\n",
    "                    ('dt', DecisionTreeClassifier())])\n",
    "\n",
    "params_pipe_DT = {'fselector__k': [data.shape[1]],\n",
    "                  'dt__criterion': ['gini', 'entropy'],\n",
    "                  'dt__max_depth': [2,3, 5, 7, 10],\n",
    "                  'dt__min_samples_split': [2, 5, 15, 20, 30]}\n",
    "\n",
    "gs_pipe_DT = GridSearchCV(estimator=pipe_DT, \n",
    "                          param_grid=params_pipe_DT, \n",
    "                          cv=cv_method,\n",
    "                          refit=True,\n",
    "                          n_jobs=-2,\n",
    "                          scoring='roc_auc',\n",
    "                          verbose=1) \n",
    "gs_pipe_DT.fit(Data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of parameters using DT model\n",
      "{'dt__criterion': 'gini', 'dt__max_depth': 5, 'dt__min_samples_split': 30, 'fselector__k': 30}\n",
      "roc_auc value fitting with best parameters\n",
      "0.9545615763546798\n"
     ]
    }
   ],
   "source": [
    "print(\"Best set of parameters using DT model\")\n",
    "print(gs_pipe_DT.best_params_)\n",
    "print(\"roc_auc value fitting with best parameters\")\n",
    "print(gs_pipe_DT.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support-Vector Machines(SVM) MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 60 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  82 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-2)]: Done 382 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-2)]: Done 882 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-2)]: Done 900 out of 900 | elapsed:   52.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipe_SVM = Pipeline([('fselector', SelectKBest(fs.mutual_info_classif)),\n",
    "                    ('svm', SVC())])\n",
    "\n",
    "params_pipe_SVM = {'fselector__k': [data.shape[1]],\n",
    "                   'svm__C': [ 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,],\n",
    "                   'svm__gamma': [1.e-03, 1.e-02, 1.e-01, 1.e+00],\n",
    "                   'svm__kernel': ['rbf','linear','poly']\n",
    "                  }\n",
    "\n",
    "gs_pipe_SVM = GridSearchCV(estimator=pipe_SVM, \n",
    "                          param_grid=params_pipe_SVM, \n",
    "                          cv=cv_method,\n",
    "                          refit=True,\n",
    "                          n_jobs=-2,\n",
    "                          scoring='roc_auc',\n",
    "                          verbose=1) \n",
    "\n",
    "gs_pipe_SVM.fit(Data_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of parameters using SVM model\n",
      "{'fselector__k': 30, 'svm__C': 10.0, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "roc_auc value fitting with best parameters\n",
      "0.9969786535303778\n"
     ]
    }
   ],
   "source": [
    "print(\"Best set of parameters using SVM model\")\n",
    "print(gs_pipe_SVM.best_params_)\n",
    "print(\"roc_auc value fitting with best parameters\")\n",
    "print(gs_pipe_SVM.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit the models with the best set of of parameters as obtained while modelling to the test data.We will then calculate the classification metrics, confusion matrix and the cross validated AUC score for the 4 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validated KNN AUC score\n",
      "0.9867598891408416\n",
      "\n",
      "Classification report for K-Nearest Neighbor\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       106\n",
      "           1       1.00      0.90      0.95        63\n",
      "\n",
      "    accuracy                           0.96       169\n",
      "   macro avg       0.97      0.95      0.96       169\n",
      "weighted avg       0.97      0.96      0.96       169\n",
      "\n",
      "\n",
      "Confusion matrix for K-Nearest Neighbor\n",
      "[[106   0]\n",
      " [  6  57]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics as metrics\n",
    "cv_method_ttest = StratifiedKFold(n_splits=3, random_state=999)\n",
    "\n",
    "cv_results_KNN = cross_val_score(estimator=gs_pipe_KNN.best_estimator_,\n",
    "                                 X=Data_test,\n",
    "                                 y=target_test, \n",
    "                                 cv=cv_method_ttest, \n",
    "                                 n_jobs=-2,\n",
    "                                 scoring='roc_auc')\n",
    "print(\"cross validated KNN AUC score\")\n",
    "print(cv_results_KNN.mean())\n",
    "pred_KNN = gs_pipe_KNN.predict(Data_test)\n",
    "print(\"\\nClassification report for K-Nearest Neighbor\") \n",
    "print(metrics.classification_report(target_test, pred_KNN))\n",
    "print(\"\\nConfusion matrix for K-Nearest Neighbor\") \n",
    "print(metrics.confusion_matrix(target_test, pred_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validated NB AUC score\n",
      "0.987893675988914\n",
      "\n",
      "\n",
      "Classification report for Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       106\n",
      "           1       0.92      0.95      0.94        63\n",
      "\n",
      "    accuracy                           0.95       169\n",
      "   macro avg       0.95      0.95      0.95       169\n",
      "weighted avg       0.95      0.95      0.95       169\n",
      "\n",
      "\n",
      "Confusion matrix for Naive Bayes\n",
      "[[101   5]\n",
      " [  3  60]]\n"
     ]
    }
   ],
   "source": [
    "Data_test_transformed = PowerTransformer().fit_transform(Data_test)\n",
    "\n",
    "cv_results_NB = cross_val_score(estimator=gs_pipe_NB.best_estimator_,\n",
    "                                X=Data_test_transformed,\n",
    "                                y=target_test, \n",
    "                                cv=cv_method_ttest, \n",
    "                                n_jobs=-2,\n",
    "                                scoring='roc_auc')\n",
    "print(\"cross validated NB AUC score\")\n",
    "print(cv_results_NB.mean())\n",
    "Data_test_transformed = PowerTransformer().fit_transform(Data_test)\n",
    "pred_NB = gs_pipe_NB.predict(Data_test_transformed)\n",
    "print(\"\\n\\nClassification report for Naive Bayes\") \n",
    "print(metrics.classification_report(target_test, pred_NB))\n",
    "print(\"\\nConfusion matrix for Naive Bayes\") \n",
    "print(metrics.confusion_matrix(target_test, pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validated DT AUC score\n",
      "0.9066956412194508\n",
      "\n",
      "\n",
      "Classification report for Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       106\n",
      "           1       0.85      0.83      0.84        63\n",
      "\n",
      "    accuracy                           0.88       169\n",
      "   macro avg       0.88      0.87      0.87       169\n",
      "weighted avg       0.88      0.88      0.88       169\n",
      "\n",
      "\n",
      "Confusion matrix for Decision Tree\n",
      "[[97  9]\n",
      " [11 52]]\n"
     ]
    }
   ],
   "source": [
    "cv_results_DT = cross_val_score(estimator=gs_pipe_DT.best_estimator_,\n",
    "                                X=Data_test,\n",
    "                                y=target_test, \n",
    "                                cv=cv_method_ttest, \n",
    "                                n_jobs=-2,\n",
    "                                scoring='roc_auc')\n",
    "print(\"cross validated DT AUC score\")\n",
    "print(cv_results_DT.mean())\n",
    "pred_DT = gs_pipe_DT.predict(Data_test)\n",
    "print(\"\\n\\nClassification report for Decision Tree\") \n",
    "print(metrics.classification_report(target_test, pred_DT))\n",
    "print(\"\\nConfusion matrix for Decision Tree\") \n",
    "print(metrics.confusion_matrix(target_test, pred_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validated SVM AUC score\n",
      "0.9918493323255229\n",
      "\n",
      "\n",
      "Classification report for SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       106\n",
      "           1       0.97      0.95      0.96        63\n",
      "\n",
      "    accuracy                           0.97       169\n",
      "   macro avg       0.97      0.97      0.97       169\n",
      "weighted avg       0.97      0.97      0.97       169\n",
      "\n",
      "\n",
      "Confusion matrix for Decision Tree\n",
      "[[104   2]\n",
      " [  3  60]]\n"
     ]
    }
   ],
   "source": [
    "cv_results_SVM = cross_val_score(estimator=gs_pipe_SVM.best_estimator_,\n",
    "                                X=Data_test,\n",
    "                                y=target_test, \n",
    "                                cv=cv_method_ttest, \n",
    "                                n_jobs=-2,\n",
    "                                scoring='roc_auc')\n",
    "print(\"cross validated SVM AUC score\")\n",
    "print(cv_results_SVM.mean())\n",
    "pred_SVM = gs_pipe_SVM.predict(Data_test)\n",
    "print(\"\\n\\nClassification report for SVM\") \n",
    "print(metrics.classification_report(target_test, pred_SVM))\n",
    "print(\"\\nConfusion matrix for Decision Tree\") \n",
    "print(metrics.confusion_matrix(target_test, pred_SVM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the AUC score we recieved after fitting the model to the test data we an see that the SVM model give us the highest and thus the best AUC score for the data. We can also observe from the performance metrics of the SVM model that it has a precision of 0.97 for predicting both the malignant and benign tumor diagnosis. The model also has a higher recall rate of 0.95 for the malignant tumor diagnosis which means that  We can also see that the AUC score is almost similar when fitted both on the training data and the testing data. Thus we can conclude that there is not much overfitting in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
